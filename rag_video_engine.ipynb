{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeGetpw92FfJgsE5xQIu9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Remache-Abderraheem/ChatbotCollab/blob/main/rag_video_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OJRRXDTW_2a0"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U openai-whisper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bd0xZb9AHuE",
        "outputId": "973e57f7-07c3-4c0d-e55c-8245d502a3a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/798.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=97e812dd91fde3ff7ed7d942f2ac734b8859b11d4e2f649278d0c98b92d52d56\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import firestore\n",
        "import whisper\n",
        "from google.cloud import storage\n",
        "from whisper.utils import get_writer\n",
        "import re\n",
        "\n",
        "whisper_bucket = \"rag-videos-engine-whisper\"\n",
        "\n",
        "def is_valid_text(text):\n",
        "  # Remove all special characters\n",
        "  cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "  # Split the cleaned text into words\n",
        "  words = cleaned_text.split()\n",
        "  # Check if the number of words exceeds 20\n",
        "  if len(words) > 30:\n",
        "      return True\n",
        "  else:\n",
        "      return False\n",
        "\n",
        "\n",
        "def download_video(blob_name) -> str:\n",
        "\n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "\n",
        "    # Replace 'your-bucket-name' with the name of your GCS bucket\n",
        "    bucket = client.bucket('rag-videos-engine-downloads')\n",
        "\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    blob.download_to_filename(\"vid.mp4\")\n",
        "\n",
        "\n",
        "\n",
        "def save_file(results, format='tsv'):\n",
        "    writer = get_writer(format, './')\n",
        "    writer(results, f'transcribe.{format}')\n",
        "\n",
        "\n",
        "def upload_file_to_bucket(bucket_name, folder_name, file_name, file_content):\n",
        "    \"\"\"Uploads a file to a Cloud Storage bucket.\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(folder_name + \"/\" + file_name)\n",
        "    blob.upload_from_string(file_content)\n",
        "\n",
        "\n",
        "# Initialize Firestore client\n",
        "db = firestore.Client(project=\"rag-videos-engine\")\n",
        "\n",
        "# Replace 'your-collection-name' with the name of your collection\n",
        "collection_name = 'video-source'\n",
        "\n",
        "docs = db.collection(collection_name).stream()\n",
        "wstatus = []\n",
        "testModel = whisper.load_model(\"tiny\")\n",
        "processingModel = whisper.load_model(\"large\")\n",
        "\n",
        "\n",
        "for doc in docs:\n",
        "  if doc.id == \"0s1UkYLvh6vfmS6VMLtx\":\n",
        "    print(doc.id)\n",
        "    break\n",
        "doc_dict = doc.to_dict()\n",
        "title_array = doc_dict.get(\"Title\")\n",
        "text = doc_dict.get(\"Text\")\n",
        "status_array = doc_dict.get(\"Status\")\n",
        "whisper_array = doc_dict.get(\"WStatus\")\n",
        "if whisper_array:\n",
        "  print(\"no more to add\")\n",
        "else:\n",
        "  index = 0\n",
        "  for title,status in zip(title_array, status_array):\n",
        "    if status == \"OK\":\n",
        "      print(title)\n",
        "      download_video(title+\".mp4\")\n",
        "      test = testModel.transcribe(\"vid.mp4\", task='translate')\n",
        "      print(test)\n",
        "      if is_valid_text(test[\"text\"]):\n",
        "          print(\"Valid text\")\n",
        "          result = processingModel.transcribe(\"vid.mp4\", task='translate')\n",
        "          types = [\"txt\", \"vtt\", \"srt\", \"tsv\", \"json\"]\n",
        "          for ftype in types:\n",
        "            save_file(result, ftype)\n",
        "          folder = doc.id+\"/\"+str(index)\n",
        "          for ftype in types:\n",
        "            with open(\"transcribe.\"+ftype, 'r') as file:\n",
        "              # Read the content of the file\n",
        "              file_content = file.read()\n",
        "            upload_file_to_bucket(whisper_bucket,folder,\"transcribe.\"+ftype,file_content)\n",
        "          wstatus.append(\"Transcribed\")\n",
        "          index+=1\n",
        "      else:\n",
        "          print(\"Invalid text\")\n",
        "          if is_valid_text(text):\n",
        "            folder = doc.id+\"/\"+str(index)\n",
        "            upload_file_to_bucket(whisper_bucket,folder,\"transcribe.txt\",text)\n",
        "            wstatus.append(\"Empty\")\n",
        "            index+=1\n",
        "          else:\n",
        "            wstatus.append(\"Unuseful\")\n",
        "            index+=1\n",
        "    else:\n",
        "      wstatus.append(\"Error\")\n",
        "      index+=1\n",
        "doc_ref = db.collection(collection_name).document(doc.id)\n",
        "doc_ref.update({\"WStatus\": wstatus})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxZG_yEs6IdC",
        "outputId": "dd4ea820-8848-43c0-b9fe-5c2eba59de7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0s1UkYLvh6vfmS6VMLtx\n",
            "More_and_more_crimes1\n",
            "{'text': \" You should know how many times it he's doing It's the song Let's go then Let me tell you See you guys In the sun Sweat! They toured at a caigy\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 3.2600000000000002, 'text': \" You should know how many times it he's doing\", 'tokens': [50364, 509, 820, 458, 577, 867, 1413, 309, 415, 311, 884, 50527], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 1, 'seek': 0, 'start': 5.4, 'end': 6.96, 'text': \" It's the song\", 'tokens': [50634, 467, 311, 264, 2153, 50712], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 2, 'seek': 0, 'start': 7.58, 'end': 8.86, 'text': \" Let's go then\", 'tokens': [50743, 961, 311, 352, 550, 50807], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 3, 'seek': 0, 'start': 8.96, 'end': 10.06, 'text': ' Let me tell you', 'tokens': [50812, 961, 385, 980, 291, 50867], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 4, 'seek': 0, 'start': 13.02, 'end': 14.82, 'text': ' See you guys', 'tokens': [51015, 3008, 291, 1074, 51105], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 5, 'seek': 0, 'start': 15.700000000000001, 'end': 17.02, 'text': ' In the sun', 'tokens': [51149, 682, 264, 3295, 51215], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 6, 'seek': 0, 'start': 26.14, 'end': 26.6, 'text': ' Sweat!', 'tokens': [51671, 29918, 267, 0, 51694], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}, {'id': 7, 'seek': 0, 'start': 26.82, 'end': 29.580000000000002, 'text': ' They toured at a caigy', 'tokens': [51705, 814, 10095, 986, 412, 257, 1335, 328, 88, 51843], 'temperature': 1.0, 'avg_logprob': -4.0058424813406805, 'compression_ratio': 1.2566371681415929, 'no_speech_prob': 0.052502818405628204}], 'language': 'ar'}\n",
            "Valid text\n",
            "More_and_more_crimes2\n",
            "{'text': ' it would have been much harder, to predict what I am considering. I no longer have to think about it. I do not have to do me this way. My Problem will be solved in a怖- merry Christmas.', 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 3.5, 'text': ' it would have been much harder,', 'tokens': [50364, 309, 576, 362, 668, 709, 6081, 11, 50539], 'temperature': 1.0, 'avg_logprob': -3.342106410435268, 'compression_ratio': 1.282758620689655, 'no_speech_prob': 0.03639892488718033}, {'id': 1, 'seek': 0, 'start': 3.5, 'end': 4.88, 'text': ' to predict what I am considering.', 'tokens': [50539, 281, 6069, 437, 286, 669, 8079, 13, 50608], 'temperature': 1.0, 'avg_logprob': -3.342106410435268, 'compression_ratio': 1.282758620689655, 'no_speech_prob': 0.03639892488718033}, {'id': 2, 'seek': 0, 'start': 4.88, 'end': 6.96, 'text': ' I no longer have to think about it.', 'tokens': [50608, 286, 572, 2854, 362, 281, 519, 466, 309, 13, 50712], 'temperature': 1.0, 'avg_logprob': -3.342106410435268, 'compression_ratio': 1.282758620689655, 'no_speech_prob': 0.03639892488718033}, {'id': 3, 'seek': 0, 'start': 6.96, 'end': 11.18, 'text': ' I do not have to do me this way.', 'tokens': [50712, 286, 360, 406, 362, 281, 360, 385, 341, 636, 13, 50923], 'temperature': 1.0, 'avg_logprob': -3.342106410435268, 'compression_ratio': 1.282758620689655, 'no_speech_prob': 0.03639892488718033}, {'id': 4, 'seek': 0, 'start': 12.540000000000001, 'end': 15.06, 'text': ' My Problem will be solved in a怖- merry Christmas.', 'tokens': [50991, 1222, 11676, 486, 312, 13041, 294, 257, 39647, 12, 41545, 5272, 13, 51117], 'temperature': 1.0, 'avg_logprob': -3.342106410435268, 'compression_ratio': 1.282758620689655, 'no_speech_prob': 0.03639892488718033}], 'language': 'ar'}\n",
            "Valid text\n",
            "More_and_more_crimes3\n",
            "{'text': \" Thank you for watching Good Good It's really messy You really is a success\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 5.4, 'text': ' Thank you for watching', 'tokens': [50364, 1044, 291, 337, 1976, 50634], 'temperature': 1.0, 'avg_logprob': -4.103812877948467, 'compression_ratio': 1.0571428571428572, 'no_speech_prob': 0.053018968552351}, {'id': 1, 'seek': 0, 'start': 5.96, 'end': 8.72, 'text': ' Good', 'tokens': [50662, 2205, 50800], 'temperature': 1.0, 'avg_logprob': -4.103812877948467, 'compression_ratio': 1.0571428571428572, 'no_speech_prob': 0.053018968552351}, {'id': 2, 'seek': 0, 'start': 9.66, 'end': 10.64, 'text': ' Good', 'tokens': [50847, 2205, 50896], 'temperature': 1.0, 'avg_logprob': -4.103812877948467, 'compression_ratio': 1.0571428571428572, 'no_speech_prob': 0.053018968552351}, {'id': 3, 'seek': 0, 'start': 15.0, 'end': 19.48, 'text': \" It's really messy\", 'tokens': [51114, 467, 311, 534, 16191, 51338], 'temperature': 1.0, 'avg_logprob': -4.103812877948467, 'compression_ratio': 1.0571428571428572, 'no_speech_prob': 0.053018968552351}, {'id': 4, 'seek': 0, 'start': 21.72, 'end': 27.8, 'text': ' You really is a success', 'tokens': [51450, 509, 534, 307, 257, 2245, 51754], 'temperature': 1.0, 'avg_logprob': -4.103812877948467, 'compression_ratio': 1.0571428571428572, 'no_speech_prob': 0.053018968552351}], 'language': 'ko'}\n",
            "Invalid text\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "update_time {\n",
              "  seconds: 1714829048\n",
              "  nanos: 346918000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}